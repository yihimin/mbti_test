{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yihimin/mbti_test/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJS7kf_impWl",
        "outputId": "4f7f5045-16d2-4409-ec6e-4c8d3a0085e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 1s (8,545 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 123630 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxR_XJdOnda-",
        "outputId": "60b5b2cb-8f33-469d-be0d-0ce08376561e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytrends\n",
            "  Downloading pytrends-4.9.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.32.3)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pytrends) (5.3.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.16.0)\n",
            "Downloading pytrends-4.9.2-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pytrends\n",
            "Successfully installed pytrends-4.9.2\n"
          ]
        }
      ],
      "source": [
        "pip install pytrends"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al2-d-A9MaUv"
      },
      "source": [
        "# 5개년도 노벨문학상 수상자를 이용한 언급량과 책 판매량의 상관관계\n",
        "\n",
        "1. 데이터 준비\n",
        "\n",
        " 1.1 노벨문학상 수상자 크롤링(위키피디아)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E9bBmPwMTuj",
        "outputId": "6dc1308d-58f5-4797-8a23-a023dd116500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터가 nobel_winners_2020s.csv에 저장되었습니다.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# 1. URL 설정\n",
        "url = \"https://ko.wikipedia.org/wiki/노벨_문학상_수상자_목록\"\n",
        "\n",
        "# 2. HTTP 요청\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
        "}\n",
        "response = requests.get(url, headers=headers)\n",
        "response.raise_for_status()  # 요청 성공 확인\n",
        "\n",
        "# 3. HTML 파싱\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# 4. 테이블 가져오기 (13번째 테이블 선택)\n",
        "tables = soup.find_all(\"table\", {\"class\": \"wikitable\"})\n",
        "if len(tables) < 13:\n",
        "    print(\"테이블 개수가 충분하지 않습니다.\")\n",
        "    exit()\n",
        "\n",
        "target_table = tables[12]  # 리스트 인덱스는 0부터 시작하므로 13번째는 인덱스 12\n",
        "\n",
        "# 5. 데이터 추출 및 저장\n",
        "rows = target_table.find_all(\"tr\")  # 테이블 내 모든 행 가져오기\n",
        "\n",
        "# 결과를 저장할 리스트\n",
        "data = []\n",
        "\n",
        "for row in rows[1:]:  # 헤더 제외\n",
        "    columns = row.find_all(\"td\")  # 각 행의 열 데이터\n",
        "    if len(columns) >= 3:  # 열이 3개 이상인 경우만 처리\n",
        "        year = columns[0].text.strip()  # 첫 번째 열: 연도\n",
        "        winner = columns[2].text.strip()  # 세 번째 열: 수상자 이름\n",
        "\n",
        "        # 2020년대 필터링\n",
        "        if year.isdigit() and 2020 <= int(year) < 2030:\n",
        "            data.append([year, winner])  # 리스트에 추가\n",
        "\n",
        "# 6. CSV 파일 저장\n",
        "output_file = \"nobel_winners_2020s.csv\"\n",
        "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8-sig\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"연도\", \"수상자\"])  # 헤더 작성\n",
        "    writer.writerows(data)  # 데이터 작성\n",
        "\n",
        "print(f\"데이터가 {output_file}에 저장되었습니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clSVRtCBSoui"
      },
      "source": [
        "\n",
        "1.2 노벨문학상 수상자 언급량 비교(구글 트렌트)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC3nLQfZVtSU",
        "outputId": "614cd11c-aa42-4c5e-89fa-8db0147ab4b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "요청 중: Han Kang, 기간: 2020-01-01 2021-12-31 (시도 1/5)\n",
            "Han Kang 데이터 수집 성공\n",
            "요청 중: Han Kang, 기간: 2022-01-01 2024-12-31 (시도 1/5)\n",
            "오류 발생: The request failed: Google returned a response with code 429\n",
            "요청 중: Han Kang, 기간: 2022-01-01 2024-12-31 (시도 2/5)\n",
            "오류 발생: The request failed: Google returned a response with code 429\n",
            "요청 중: Han Kang, 기간: 2022-01-01 2024-12-31 (시도 3/5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Han Kang 데이터 수집 성공\n",
            "요청 중: Jon Fosse, 기간: 2020-01-01 2021-12-31 (시도 1/5)\n",
            "Jon Fosse 데이터 수집 성공\n",
            "요청 중: Jon Fosse, 기간: 2022-01-01 2024-12-31 (시도 1/5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jon Fosse 데이터 수집 성공\n",
            "요청 중: Annie Ernaux, 기간: 2020-01-01 2021-12-31 (시도 1/5)\n",
            "Annie Ernaux 데이터 수집 성공\n",
            "요청 중: Annie Ernaux, 기간: 2022-01-01 2024-12-31 (시도 1/5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annie Ernaux 데이터 수집 성공\n",
            "요청 중: Abdulrazak Gurnah, 기간: 2020-01-01 2021-12-31 (시도 1/5)\n",
            "오류 발생: The request failed: Google returned a response with code 429\n",
            "요청 중: Abdulrazak Gurnah, 기간: 2020-01-01 2021-12-31 (시도 2/5)\n",
            "오류 발생: The request failed: Google returned a response with code 429\n",
            "요청 중: Abdulrazak Gurnah, 기간: 2020-01-01 2021-12-31 (시도 3/5)\n",
            "오류 발생: The request failed: Google returned a response with code 429\n",
            "요청 중: Abdulrazak Gurnah, 기간: 2020-01-01 2021-12-31 (시도 4/5)\n",
            "Abdulrazak Gurnah 데이터 수집 성공\n",
            "요청 중: Abdulrazak Gurnah, 기간: 2022-01-01 2024-12-31 (시도 1/5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abdulrazak Gurnah 데이터 수집 성공\n",
            "요청 중: Louise Glück, 기간: 2020-01-01 2021-12-31 (시도 1/5)\n",
            "Louise Glück 데이터 수집 성공\n",
            "요청 중: Louise Glück, 기간: 2022-01-01 2024-12-31 (시도 1/5)\n",
            "오류 발생: The request failed: Google returned a response with code 429\n",
            "요청 중: Louise Glück, 기간: 2022-01-01 2024-12-31 (시도 2/5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Louise Glück 데이터 수집 성공\n",
            "데이터 저장 완료: nobel_trends.csv\n"
          ]
        }
      ],
      "source": [
        "from pytrends.request import TrendReq\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Google Trends API 연결\n",
        "pytrends = TrendReq(hl='en-US', tz=360)\n",
        "\n",
        "# 키워드 설정\n",
        "keywords = [\"Han Kang\", \"Jon Fosse\", \"Annie Ernaux\", \"Abdulrazak Gurnah\", \"Louise Glück\"]\n",
        "\n",
        "# 타임프레임 분할\n",
        "timeframes = [\"2020-01-01 2021-12-31\", \"2022-01-01 2024-12-31\"]\n",
        "\n",
        "# 데이터 저장용 DataFrame 초기화\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "# 데이터 요청\n",
        "for keyword in keywords:\n",
        "    for timeframe in timeframes:\n",
        "        success = False\n",
        "        attempts = 0\n",
        "        while not success and attempts < 5:  # 최대 5회 재시도\n",
        "            try:\n",
        "                print(f\"요청 중: {keyword}, 기간: {timeframe} (시도 {attempts + 1}/5)\")\n",
        "                pytrends.build_payload([keyword], cat=0, timeframe=timeframe, geo='', gprop='')\n",
        "                data = pytrends.interest_over_time()\n",
        "                if not data.empty:\n",
        "                    print(f\"{keyword} 데이터 수집 성공\")\n",
        "                    data = data[[keyword]]  # 현재 키워드 데이터만 선택\n",
        "                    all_data = pd.concat([all_data, data], axis=1)  # 기존 데이터와 병합\n",
        "                else:\n",
        "                    print(f\"{keyword}에 대한 데이터가 없습니다.\")\n",
        "                success = True\n",
        "            except Exception as e:\n",
        "                print(f\"오류 발생: {e}\")\n",
        "                attempts += 1\n",
        "                time.sleep(60)  # 실패 시 대기 시간 증가\n",
        "        time.sleep(60)  # 각 키워드 요청 간 대기 시간\n",
        "\n",
        "# 데이터 저장\n",
        "output_file = \"nobel_trends.csv\"\n",
        "all_data.to_csv(output_file, encoding=\"utf-8-sig\")\n",
        "print(f\"데이터 저장 완료: {output_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT3dZrQuRAkl"
      },
      "source": [
        "1.3 노벨문학상 수상자 책 판매순위"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z9K3hQoR7C6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc9638c-a3c1-4e49-dfef-ca5a11407d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "책 데이터를 찾을 수 없습니다.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# 교보문고 검색 URL 예제\n",
        "base_url = \"https://search.kyobobook.co.kr/search?keyword={}&gbCode=TOT&target=total\"\n",
        "author = \"한강\"\n",
        "\n",
        "response = requests.get(base_url.format(author), headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# 교보문고 데이터 크롤링 예제\n",
        "books = soup.find_all(\"div\", class_=\"title\")\n",
        "if books:\n",
        "    for book in books[:5]:  # 5개의 데이터만 예제로 출력\n",
        "        title = book.text.strip()\n",
        "        print(f\"책 제목: {title}\")\n",
        "else:\n",
        "    print(\"책 데이터를 찾을 수 없습니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 데이터 정규화\n",
        "\n",
        "\n",
        "   2.1 위 데이터를 수집하여 하나의 CSV 파일로 만듦\n",
        "\n"
      ],
      "metadata": {
        "id": "L14osusN6lm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "winners_data.rename(columns={\"수상자\": \"작가\"}, inplace=True)\n"
      ],
      "metadata": {
        "id": "w_Aqat_i-KKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 열 이름 변경 확인\n",
        "print(winners_data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvtZG4KM-hvO",
        "outputId": "c8bb5314-af61-4fce-949e-d0afc2b495d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['연도', '작가'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sales_data.head())  # sales_data 데이터프레임 확인\n",
        "print(sales_data.columns)  # 열 이름 확인\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AInM5ALf_YkL",
        "outputId": "42f84633-608b-4031-9a89-c16506064cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [작가, 책 제목, 상세 링크, 판매 정보]\n",
            "Index: []\n",
            "Index(['작가', '책 제목', '상세 링크', '판매 정보'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 임시 판매 데이터 생성\n",
        "sales_data = pd.DataFrame({\n",
        "    \"작가\": [\"루이즈 글릭\", \"압둘라자크 구르나\", \"아니 에르노\", \"욘 포세\", \"한강\"],\n",
        "    \"책 제목\": [\"책1\", \"책2\", \"책3\", \"책4\", \"책5\"],\n",
        "    \"상세 링크\": [\"링크1\", \"링크2\", \"링크3\", \"링크4\", \"링크5\"],\n",
        "    \"판매 정보\": [500, 300, 400, 450, 600]\n",
        "})\n"
      ],
      "metadata": {
        "id": "t4M5vcBB-Mlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 병합 진행 (sales_data, winners_data)\n",
        "merged_data = pd.merge(winners_data, sales_data, on=\"작가\", how=\"inner\")\n",
        "print(merged_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "pmyr9jU3_Pbx",
        "outputId": "9a60d402-a87a-4edc-af27-ef170719a969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'작가'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-5ab4993aed14>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 병합 진행 (sales_data, winners_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmerged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwinners_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msales_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"작가\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1308\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '작가'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 열 이름 정리 (열에서 .1 제거)\n",
        "trends_data.columns = trends_data.columns.str.replace(\".1\", \"\", regex=False)\n",
        "\n",
        "# 데이터프레임 변환 (long format)\n",
        "trends_long = pd.melt(trends_data, id_vars=[\"date\"], var_name=\"작가\", value_name=\"언급량\")\n",
        "\n",
        "# 결측값 제거\n",
        "trends_long.dropna(subset=[\"언급량\"], inplace=True)\n",
        "\n",
        "print(trends_long.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shiBMgEc-PAO",
        "outputId": "705ca5ef-81db-4c97-c652-427b4fd6257a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         date        작가   언급량\n",
            "0  2019-12-29  Han Kang  17.0\n",
            "1  2020-01-05  Han Kang  19.0\n",
            "2  2020-01-12  Han Kang  19.0\n",
            "3  2020-01-19  Han Kang  29.0\n",
            "4  2020-01-26  Han Kang  35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CSV 파일 경로 설정\n",
        "sales_file = \"/content/nobel_books_sales.csv\"\n",
        "trends_file = \"/content/nobel_trends.csv\"\n",
        "winners_file = \"/content/nobel_winners_2020s.csv\"\n",
        "\n",
        "# 데이터 불러오기\n",
        "sales_data = pd.read_csv(sales_file)\n",
        "trends_data = pd.read_csv(trends_file)\n",
        "winners_data = pd.read_csv(winners_file)\n",
        "\n",
        "# 데이터 확인\n",
        "print(\"Sales Data:\")\n",
        "print(sales_data.head())\n",
        "print(\"\\nTrends Data:\")\n",
        "print(trends_data.head())\n",
        "print(\"\\nWinners Data:\")\n",
        "print(winners_data.head())\n",
        "\n",
        "# 데이터 병합 (예: 작가 이름을 기준으로 병합)\n",
        "merged_data = pd.merge(winners_data, sales_data, on=\"작가\", how=\"inner\")\n",
        "merged_data = pd.merge(merged_data, trends_data, on=\"작가\", how=\"inner\")\n",
        "\n",
        "# 병합 결과 확인\n",
        "print(\"\\nMerged Data:\")\n",
        "print(merged_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 997
        },
        "id": "MfQ9WVj663Ux",
        "outputId": "3a5b124f-3e5e-4a79-a540-2b1ad4407b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sales Data:\n",
            "Empty DataFrame\n",
            "Columns: [작가, 책 제목, 상세 링크, 판매 정보]\n",
            "Index: []\n",
            "\n",
            "Trends Data:\n",
            "         date  Han Kang  Han Kang.1  Jon Fosse  Jon Fosse.1  Annie Ernaux  \\\n",
            "0  2019-12-29      17.0         NaN       25.0          NaN           9.0   \n",
            "1  2020-01-05      19.0         NaN       28.0          NaN          11.0   \n",
            "2  2020-01-12      19.0         NaN       23.0          NaN           9.0   \n",
            "3  2020-01-19      29.0         NaN       31.0          NaN           8.0   \n",
            "4  2020-01-26      35.0         NaN       22.0          NaN           8.0   \n",
            "\n",
            "   Annie Ernaux.1  Abdulrazak Gurnah  Abdulrazak Gurnah.1  Louise Glück  \\\n",
            "0             NaN                0.0                  NaN           0.0   \n",
            "1             NaN                0.0                  NaN           0.0   \n",
            "2             NaN                0.0                  NaN           0.0   \n",
            "3             NaN                0.0                  NaN           0.0   \n",
            "4             NaN                0.0                  NaN           0.0   \n",
            "\n",
            "   Louise Glück.1  \n",
            "0             NaN  \n",
            "1             NaN  \n",
            "2             NaN  \n",
            "3             NaN  \n",
            "4             NaN  \n",
            "\n",
            "Winners Data:\n",
            "     연도        수상자\n",
            "0  2020     루이즈 글릭\n",
            "1  2021  압둘라자크 구르나\n",
            "2  2022     아니 에르노\n",
            "3  2023       욘 포세\n",
            "4  2024         한강\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'작가'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-9386e298dcb1>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 데이터 병합 (예: 작가 이름을 기준으로 병합)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmerged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwinners_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msales_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"작가\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1308\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '작가'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 데이터를 정규화 함"
      ],
      "metadata": {
        "id": "MYj_Nj6d70QD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# 정규화 대상 열 선택\n",
        "columns_to_normalize = [\"언급량\", \"판매량\"]  # 실제 열 이름으로 대체\n",
        "\n",
        "# 정규화 도구 초기화\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# 정규화 수행\n",
        "merged_data[columns_to_normalize] = scaler.fit_transform(merged_data[columns_to_normalize])\n",
        "\n",
        "print(merged_data.head())\n"
      ],
      "metadata": {
        "id": "vzDUpEKV71cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_data.describe())"
      ],
      "metadata": {
        "id": "RHKFRYiD8HIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data.to_csv(\"normalized_data.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"정규화된 데이터가 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "GqSLhYUy8JP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 데이터 분석"
      ],
      "metadata": {
        "id": "plraIh6w8DG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# 1. 데이터 불러오기\n",
        "# 언급량과 판매량 데이터를 병합한 CSV 파일 로드\n",
        "data = pd.read_csv(\"normalized_data.csv\")  # 정규화된 데이터를 저장한 파일 경로\n",
        "\n",
        "# 데이터 확인\n",
        "print(data.head())\n",
        "\n",
        "# 2. Pearson 상관계수 계산\n",
        "# 언급량과 판매량 열 이름을 확인 후 수정하세요.\n",
        "mention_column = \"언급량\"  # 언급량 컬럼 이름\n",
        "sales_column = \"판매량\"    # 판매량 컬럼 이름\n",
        "\n",
        "# 상관계수 계산\n",
        "correlation, p_value = pearsonr(data[mention_column], data[sales_column])\n",
        "print(f\"Pearson 상관계수: {correlation}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# 3. 시간적 추이를 선 그래프로 시각화\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data[\"날짜\"], data[mention_column], label=\"언급량\", marker=\"o\")  # 날짜 기준으로 언급량\n",
        "plt.plot(data[\"날짜\"], data[sales_column], label=\"판매량\", marker=\"s\")    # 날짜 기준으로 판매량\n",
        "plt.xlabel(\"날짜\")\n",
        "plt.ylabel(\"값\")\n",
        "plt.title(\"언급량과 판매량의 시간적 추이\")\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)  # 날짜 라벨 회전\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. 언급량과 판매량 간의 상관관계를 산점도로 시각화\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data[mention_column], data[sales_column], alpha=0.7)\n",
        "plt.xlabel(\"언급량\")\n",
        "plt.ylabel(\"판매량\")\n",
        "plt.title(\"언급량과 판매량 간의 상관관계\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "n3ECGh-u8XyO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTJ2QKDE2Pl4byS21kXInz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}